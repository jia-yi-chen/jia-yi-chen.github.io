---
---

# Thesis


# SVR


@InProceedings{chen2023,
  title={On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval}, 
  author={Jiayi Chen and Hanjun Dai and Bo Dai and Aidong Zhang and Wei Wei},
  year={2023},
  pdf = "emnlp.pdf",
  selected={true},
  preview="fewvder.png",
  booktitle = {EMNLP Findings},
  abstract = {Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a few-shot manner. However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents. To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task. The challenges lie in the uniqueness of the label space for each task and the increased complexity of out-of-distribution (OOD) contents. To tackle this novel task, we present a task-aware meta-learning based framework, with a central focus on achieving effective task personalization that distinguishes between in-task and out-of-task distribution. Specifically, we adopt a hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost future research in the field of entity-level few-shot VDER. Experimental results demonstrate our approaches significantly improve the robustness of popular meta-learning baselines.}
}


@InProceedings{yu2023documentnet,
  title={DocumentNet: Bridging the Data Gap in Document Pre-Training}, 
  author={Lijun Yu and Jin Miao and Xiaoyu Sun and Jiayi Chen and Alexander G. Hauptmann and Hanjun Dai and Wei Wei},
  year={2023},
  eprint={2306.08937},
  booktitle={arXiv},
  primaryClass={cs.CL},
  selected={true},
  url = "https://arxiv.org/abs/2306.08937",
  preview="uniformer.png",
  booktitle = {EMNLP Industry Track},
  abstract = {Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.}
}

@InProceedings{10.1145/3580305.3599448,
  author = {Chen, Jiayi and Zhang, Aidong},
  title = {On Hierarchical Disentanglement of Interactive Behaviors for Multimodal Spatiotemporal Data with Incompleteness},
  year = {2023},
  isbn = {9798400701030},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3580305.3599448},
  abstract = {Multimodal spatiotemporal data (MST) consists of multiple simultaneous spatiotemporal modalities that interact with each other in a dynamic manner. Due to the complexity of MST and the recent desire for the explainability of artificial intelligent systems, disentangled representation learning for MST (DisentMST) has become a significant task, which aims to learn disentangled representations that can expose the underlying spatial semantics, temporal dynamic patterns, and inter-modality interaction modes of the complex MST. One limitation of existing approaches is that they might fail to tolerate the real-world incomplete MST data, where missing information might break the cross-modal spatiotemporal dynamics and bring noise and ambiguity to the learning process. Another limitation is that no existing work systematically reveals the structure of different types of disentangled information. To tackle the two limitations, we define a novel two-level hierarchically structured disentanglement task for MST, which reveals informative and structured disentangled representations for MST as well as digests the real-world MST with incompleteness. We propose a new framework, BiDisentMST, which leverages Gaussian Processes and Graph Factorization on the latent space to achieve our purposes. The experimental results demonstrate the effectiveness of our proposed framework compared with baselines with respect to disentanglement and imputation results.},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD): Research Track},
  pages = {213–225},
  numpages = {13},
  keywords = {graph factorization, autoencoder, missing data, multiple modalities, spatiotemporal disentanglement},
  location = {Long Beach, CA, USA},
  series = {KDD '23},
  preview="kdd23.png",
  abbr="KDD",
  selected={true},
  url = "https://doi.org/10.1145/3580305.3599448",
}

@InProceedings{10.1145/3534678.3539384,
  author = {Chen, Jiayi and Zhang, Aidong},
  title = {FedMSplit: Correlation-Adaptive Federated Multi-Task Learning across Multimodal Split Networks},
  year = {2022},
  isbn = {9781450393850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  
  doi = {10.1145/3534678.3539384},
  abstract = {With the advancement of data collection techniques, end users are interested in how different types of data can collaborate to improve our life experiences. Multimodal Federated Learning (MFL) is an emerging area allowing many distributed clients, each of which can collect data from multiple types of sensors, to participate in the training of some multimodal data-related models without sharing their data. In this paper, we address a novel challenging issue in MFL, the modality incongruity, where clients may have heterogeneous setups of sensors and their local data consists of different combinations of modalities. With the modality incongruity, clients may solve different tasks on different parameter spaces, which escalates the difficulties in dealing with the statistical heterogeneity problem of federated learning; also, it would be hard to perform accurate model aggregation across different types of clients. To tackle these challenges, in this work, we propose the FedMSplit framework, which allows federated training over multimodal distributed data without assuming similar active sensors in all clients. The key idea is to employ a dynamic and multi-view graph structure to adaptively capture the correlations amongst multimodal client models. More specifically, we split client models into smaller shareable blocks and allow each type of blocks to provide a specific view on client relationships. With the graph representation, the underlying correlations between clients can be captured as the edge features in the multi-view graph, and then be utilized to promote local model relations through the neighborhood message passing in the graph. Our experimental results demonstrate the effectiveness of our method under different sensor setups with statistical heterogeneity.},
  booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD): Research Track},
  pages = {87–96},
  numpages = {10},
  keywords = {federated learning, multitask learning, modality incongruity},
  location = {Washington DC, USA},
  series = {KDD '22},
  abbr="KDD",
  preview="kdd22.png",
  selected={true},
  url = "https://doi.org/10.1145/3534678.3539384",
}

@inproceedings{10.1145/3485447.3512033,
  author = {Chen, Jiayi and Zhang, Aidong},
  title = {Topological Transduction for Hybrid Few-Shot Learning},
  year = {2022},
  isbn = {9781450390965},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = "https://doi.org/10.1145/3485447.3512033",
  doi = {10.1145/3485447.3512033},
  abstract = {Digging informative knowledge and analyzing contents from the internet is a challenging task as web data may contain new concepts that are lack of sufficient labeled data as well as could be multimodal. Few-shot learning (FSL) has attracted significant research attention for dealing with scarcely labeled concepts. However, existing FSL algorithms have assumed a uniform task setting such that all samples in a few-shot task share a common feature space. Yet in the real web applications, it is usually the case that a task may involve multiple input feature spaces due to the heterogeneity of source data, that is, the few labeled samples in a task may be further divided and belong to different feature spaces, namely hybrid few-shot learning (hFSL). The hFSL setting results in a hybrid number of shots per class in each space and aggravates the data scarcity challenge as the number of training samples per class in each space is reduced. To alleviate these challenges, we propose the Task-adaptive Topological Transduction Network, namely TopoNet, which trains a heterogeneous graph-based transductive meta-learner that can combine information from both labeled and unlabeled data to enrich the knowledge about the task-specific data distribution and multi-space relationships. Specifically, we model the underlying data relationships of the few-shot task in a node-heterogeneous multi-relation graph, and then the meta-learner adapts to each task’s multi-space relationships as well as its inter- and intra-class data relationships, through an edge-enhanced heterogeneous graph neural network. Our experiments compared with existing approaches demonstrate the effectiveness of our method.},
  booktitle = {Proceedings of the ACM Web Conference 2022 (WWW'22): Research Track},
  pages = {3134–3142},
  numpages = {9},
  keywords = {graph neural networks, few-shot learning, semi-supervised learning, multimodal content analysis},
  location = {Virtual Event, Lyon, France},
  series = {WWW '22},
  preview="toponet.png",
  selected={true},

}

@inproceedings{10.1145/3459637.3482262,
  author = {Chen, Jiayi and Zhang, Aidong},
  title = {HetMAML: Task-Heterogeneous Model-Agnostic Meta-Learning for Few-Shot Learning Across Modalities},
  year = {2021},
  isbn = {9781450384469},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3459637.3482262},
  doi = {10.1145/3459637.3482262},
  abstract = {Most of existing gradient-based meta-learning approaches to few-shot learning assume that all tasks have the same input feature space. However, in the real world scenarios, there are many cases that the input structures of tasks can be different, that is, different tasks may vary in the number of input modalities or data types. Existing meta-learners cannot handle the heterogeneous task distribution (HTD) as there is not only global meta-knowledge shared across tasks but also type-specific knowledge that distinguishes each type of tasks. To deal with task heterogeneity and promote fast within-task adaptions for each type of tasks, in this paper, we propose HetMAML, a task-heterogeneous model-agnostic meta-learning framework, which can capture both the type-specific and globally shared knowledge and can achieve the balance between knowledge customization and generalization. Specifically, we design a multi-channel backbone module that encodes the input of each type of tasks into the same length sequence of modality-specific embeddings. Then, we propose a task-aware iterative feature aggregation network which can automatically take into account the context of task-specific input structures and adaptively project the heterogeneous input spaces to the same lower-dimensional embedding space of concepts. Our experiments on six task-heterogeneous datasets demonstrate that HetMAML successfully leverages type-specific and globally shared meta-parameters for heterogeneous tasks and achieves fast within-task adaptions for each type of tasks.},
  booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management (CIKM'21): Full Paper},
  pages = {191–200},
  numpages = {10},
  keywords = {multimodality, task heterogeneity, meta-learning, few-shot learning},
  location = {Virtual Event, Queensland, Australia},
  series = {CIKM '21},
  preview="hetmaml.png",
  selected={true},
}

@inproceedings{10.1145/3394486.3403182,
  author = {Chen, Jiayi and Zhang, Aidong},
  title = {HGMF: Heterogeneous Graph-Based Fusion for Multimodal Data with Incompleteness},
  year = {2020},
  isbn = {9781450379984},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = "https://dl.acm.org/doi/10.1145/3394486.3403182",
  doi = {10.1145/3394486.3403182},
  abstract = {With the advances in data collection techniques, large amounts of multimodal data collected from multiple sources are becoming available. Such multimodal data can provide complementary information that can reveal fundamental characteristics of real-world subjects. Thus, multimodal machine learning has become an active research area. Extensive works have been developed to exploit multimodal interactions and integrate multi-source information. However, multimodal data in the real world usually comes with missing modalities due to various reasons, such as sensor damage, data corruption, and human mistakes in recording. Effectively integrating and analyzing multimodal data with incompleteness remains a challenging problem. We propose a Heterogeneous Graph-based Multimodal Fusion (HGMF) approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure. The proposed approach develops a unique strategy for learning on incomplete multimodal data without data deletion or data imputation. More specifically, we construct a heterogeneous hypernode graph to model the multimodal data having different combinations of missing modalities, and then we formulate a graph neural network based transductive learning framework to project the heterogeneous incomplete data onto a unified embedding space, and multi-modalities are fused along the way. The learning framework captures modality interactions from available data, and leverages the relationships between different incompleteness patterns. Our experimental results demonstrate that the proposed method outperforms existing graph-based as well as non-graph based baselines on three different datasets.},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining (KDD): Research Track},
  pages = {1295–1305},
  numpages = {11},
  keywords = {multimodal fusion, graph neural networks, data incompleteness, heterogeneous graph, missing modalities},
  location = {Virtual Event, CA, USA},
  abbr="KDD",
  preview="kdd20.png",
  selected={true},
}