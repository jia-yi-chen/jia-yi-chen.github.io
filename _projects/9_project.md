---
layout: page
title: Multimodal Information Extraction
time: Summer 2022
tag: 
description: Visually-rich Documents; Transformers; BERT; Few-shot Learning; Meta Learning
img: assets/img/projects/fewvder1.jpg
importance: 1
category: Natural Language Processing
---


#### Time:
Summer 2022


#### Project: Efficient Few-shot Multimodal Information Extraction in Visually-rich Documents

Visually-rich documents consist of three modalities (language, image, and layout structure of contents). The task is to harness meta-knowledge to accelerate the learning process of 
- understanding new document types given a pre-trained text-image Large Language Model; 
- localizing rarely-occurred key information types from out-of-distribution information.


#### Accomplishment: 
Successfully published two research papers.

#### Related Skills:

- NLP: Entity retrieval, Multimodal Transformer-based Large Language Models
- Tools: Tensorflow, Jax, seqeval
- Languages: Python


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/projects/fvder3.png" title="3d-to-2d Art dataset" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/projects/fvder.jpg" title="3d-to-2d Art dataset" class="img-fluid rounded z-depth-1" %}
    </div>
</div>



#### Collaborators:
- Mentors: Hanjun Dai (Google DeepMind), Bo Dai (Google DeepMind), Wei Wei (Cloud DocAI & Core ML App)
